{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mood Detection yt.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRN30j0sfJ-J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49cdjUcfpDgk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "d609810a-f6ff-4316-a573-ddbd33ce42d5"
      },
      "source": [
        "df = pd.read_csv(\"drive/My Drive/aithon2020_level2_traning.csv\")\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emotion</th>\n",
              "      <th>pixel_0</th>\n",
              "      <th>pixel_1</th>\n",
              "      <th>pixel_2</th>\n",
              "      <th>pixel_3</th>\n",
              "      <th>pixel_4</th>\n",
              "      <th>pixel_5</th>\n",
              "      <th>pixel_6</th>\n",
              "      <th>pixel_7</th>\n",
              "      <th>pixel_8</th>\n",
              "      <th>pixel_9</th>\n",
              "      <th>pixel_10</th>\n",
              "      <th>pixel_11</th>\n",
              "      <th>pixel_12</th>\n",
              "      <th>pixel_13</th>\n",
              "      <th>pixel_14</th>\n",
              "      <th>pixel_15</th>\n",
              "      <th>pixel_16</th>\n",
              "      <th>pixel_17</th>\n",
              "      <th>pixel_18</th>\n",
              "      <th>pixel_19</th>\n",
              "      <th>pixel_20</th>\n",
              "      <th>pixel_21</th>\n",
              "      <th>pixel_22</th>\n",
              "      <th>pixel_23</th>\n",
              "      <th>pixel_24</th>\n",
              "      <th>pixel_25</th>\n",
              "      <th>pixel_26</th>\n",
              "      <th>pixel_27</th>\n",
              "      <th>pixel_28</th>\n",
              "      <th>pixel_29</th>\n",
              "      <th>pixel_30</th>\n",
              "      <th>pixel_31</th>\n",
              "      <th>pixel_32</th>\n",
              "      <th>pixel_33</th>\n",
              "      <th>pixel_34</th>\n",
              "      <th>pixel_35</th>\n",
              "      <th>pixel_36</th>\n",
              "      <th>pixel_37</th>\n",
              "      <th>pixel_38</th>\n",
              "      <th>...</th>\n",
              "      <th>pixel_2264</th>\n",
              "      <th>pixel_2265</th>\n",
              "      <th>pixel_2266</th>\n",
              "      <th>pixel_2267</th>\n",
              "      <th>pixel_2268</th>\n",
              "      <th>pixel_2269</th>\n",
              "      <th>pixel_2270</th>\n",
              "      <th>pixel_2271</th>\n",
              "      <th>pixel_2272</th>\n",
              "      <th>pixel_2273</th>\n",
              "      <th>pixel_2274</th>\n",
              "      <th>pixel_2275</th>\n",
              "      <th>pixel_2276</th>\n",
              "      <th>pixel_2277</th>\n",
              "      <th>pixel_2278</th>\n",
              "      <th>pixel_2279</th>\n",
              "      <th>pixel_2280</th>\n",
              "      <th>pixel_2281</th>\n",
              "      <th>pixel_2282</th>\n",
              "      <th>pixel_2283</th>\n",
              "      <th>pixel_2284</th>\n",
              "      <th>pixel_2285</th>\n",
              "      <th>pixel_2286</th>\n",
              "      <th>pixel_2287</th>\n",
              "      <th>pixel_2288</th>\n",
              "      <th>pixel_2289</th>\n",
              "      <th>pixel_2290</th>\n",
              "      <th>pixel_2291</th>\n",
              "      <th>pixel_2292</th>\n",
              "      <th>pixel_2293</th>\n",
              "      <th>pixel_2294</th>\n",
              "      <th>pixel_2295</th>\n",
              "      <th>pixel_2296</th>\n",
              "      <th>pixel_2297</th>\n",
              "      <th>pixel_2298</th>\n",
              "      <th>pixel_2299</th>\n",
              "      <th>pixel_2300</th>\n",
              "      <th>pixel_2301</th>\n",
              "      <th>pixel_2302</th>\n",
              "      <th>pixel_2303</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Fear</td>\n",
              "      <td>231</td>\n",
              "      <td>212</td>\n",
              "      <td>156</td>\n",
              "      <td>164</td>\n",
              "      <td>174</td>\n",
              "      <td>138</td>\n",
              "      <td>161</td>\n",
              "      <td>173</td>\n",
              "      <td>182</td>\n",
              "      <td>200</td>\n",
              "      <td>106</td>\n",
              "      <td>38</td>\n",
              "      <td>39</td>\n",
              "      <td>74</td>\n",
              "      <td>138</td>\n",
              "      <td>161</td>\n",
              "      <td>164</td>\n",
              "      <td>179</td>\n",
              "      <td>190</td>\n",
              "      <td>201</td>\n",
              "      <td>210</td>\n",
              "      <td>216</td>\n",
              "      <td>220</td>\n",
              "      <td>224</td>\n",
              "      <td>222</td>\n",
              "      <td>218</td>\n",
              "      <td>216</td>\n",
              "      <td>213</td>\n",
              "      <td>217</td>\n",
              "      <td>220</td>\n",
              "      <td>220</td>\n",
              "      <td>218</td>\n",
              "      <td>217</td>\n",
              "      <td>212</td>\n",
              "      <td>174</td>\n",
              "      <td>160</td>\n",
              "      <td>162</td>\n",
              "      <td>160</td>\n",
              "      <td>139</td>\n",
              "      <td>...</td>\n",
              "      <td>225</td>\n",
              "      <td>220</td>\n",
              "      <td>215</td>\n",
              "      <td>207</td>\n",
              "      <td>199</td>\n",
              "      <td>167</td>\n",
              "      <td>108</td>\n",
              "      <td>151</td>\n",
              "      <td>122</td>\n",
              "      <td>88</td>\n",
              "      <td>71</td>\n",
              "      <td>84</td>\n",
              "      <td>120</td>\n",
              "      <td>127</td>\n",
              "      <td>105</td>\n",
              "      <td>76</td>\n",
              "      <td>71</td>\n",
              "      <td>78</td>\n",
              "      <td>90</td>\n",
              "      <td>106</td>\n",
              "      <td>123</td>\n",
              "      <td>146</td>\n",
              "      <td>155</td>\n",
              "      <td>148</td>\n",
              "      <td>130</td>\n",
              "      <td>141</td>\n",
              "      <td>119</td>\n",
              "      <td>69</td>\n",
              "      <td>54</td>\n",
              "      <td>89</td>\n",
              "      <td>104</td>\n",
              "      <td>138</td>\n",
              "      <td>152</td>\n",
              "      <td>122</td>\n",
              "      <td>114</td>\n",
              "      <td>101</td>\n",
              "      <td>97</td>\n",
              "      <td>88</td>\n",
              "      <td>110</td>\n",
              "      <td>152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Fear</td>\n",
              "      <td>55</td>\n",
              "      <td>55</td>\n",
              "      <td>55</td>\n",
              "      <td>55</td>\n",
              "      <td>55</td>\n",
              "      <td>54</td>\n",
              "      <td>60</td>\n",
              "      <td>68</td>\n",
              "      <td>54</td>\n",
              "      <td>85</td>\n",
              "      <td>151</td>\n",
              "      <td>163</td>\n",
              "      <td>170</td>\n",
              "      <td>179</td>\n",
              "      <td>181</td>\n",
              "      <td>185</td>\n",
              "      <td>188</td>\n",
              "      <td>188</td>\n",
              "      <td>191</td>\n",
              "      <td>196</td>\n",
              "      <td>189</td>\n",
              "      <td>194</td>\n",
              "      <td>198</td>\n",
              "      <td>197</td>\n",
              "      <td>195</td>\n",
              "      <td>194</td>\n",
              "      <td>190</td>\n",
              "      <td>193</td>\n",
              "      <td>195</td>\n",
              "      <td>184</td>\n",
              "      <td>175</td>\n",
              "      <td>172</td>\n",
              "      <td>161</td>\n",
              "      <td>159</td>\n",
              "      <td>158</td>\n",
              "      <td>159</td>\n",
              "      <td>147</td>\n",
              "      <td>136</td>\n",
              "      <td>137</td>\n",
              "      <td>...</td>\n",
              "      <td>64</td>\n",
              "      <td>90</td>\n",
              "      <td>120</td>\n",
              "      <td>121</td>\n",
              "      <td>127</td>\n",
              "      <td>136</td>\n",
              "      <td>128</td>\n",
              "      <td>150</td>\n",
              "      <td>154</td>\n",
              "      <td>157</td>\n",
              "      <td>155</td>\n",
              "      <td>167</td>\n",
              "      <td>172</td>\n",
              "      <td>174</td>\n",
              "      <td>173</td>\n",
              "      <td>171</td>\n",
              "      <td>167</td>\n",
              "      <td>163</td>\n",
              "      <td>164</td>\n",
              "      <td>160</td>\n",
              "      <td>155</td>\n",
              "      <td>148</td>\n",
              "      <td>148</td>\n",
              "      <td>144</td>\n",
              "      <td>130</td>\n",
              "      <td>126</td>\n",
              "      <td>124</td>\n",
              "      <td>116</td>\n",
              "      <td>118</td>\n",
              "      <td>110</td>\n",
              "      <td>90</td>\n",
              "      <td>83</td>\n",
              "      <td>77</td>\n",
              "      <td>53</td>\n",
              "      <td>37</td>\n",
              "      <td>34</td>\n",
              "      <td>30</td>\n",
              "      <td>34</td>\n",
              "      <td>30</td>\n",
              "      <td>57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sad</td>\n",
              "      <td>20</td>\n",
              "      <td>17</td>\n",
              "      <td>19</td>\n",
              "      <td>21</td>\n",
              "      <td>25</td>\n",
              "      <td>38</td>\n",
              "      <td>42</td>\n",
              "      <td>42</td>\n",
              "      <td>46</td>\n",
              "      <td>54</td>\n",
              "      <td>56</td>\n",
              "      <td>62</td>\n",
              "      <td>63</td>\n",
              "      <td>66</td>\n",
              "      <td>82</td>\n",
              "      <td>108</td>\n",
              "      <td>118</td>\n",
              "      <td>130</td>\n",
              "      <td>139</td>\n",
              "      <td>134</td>\n",
              "      <td>132</td>\n",
              "      <td>126</td>\n",
              "      <td>113</td>\n",
              "      <td>97</td>\n",
              "      <td>126</td>\n",
              "      <td>148</td>\n",
              "      <td>157</td>\n",
              "      <td>161</td>\n",
              "      <td>155</td>\n",
              "      <td>154</td>\n",
              "      <td>154</td>\n",
              "      <td>164</td>\n",
              "      <td>189</td>\n",
              "      <td>204</td>\n",
              "      <td>194</td>\n",
              "      <td>168</td>\n",
              "      <td>180</td>\n",
              "      <td>188</td>\n",
              "      <td>214</td>\n",
              "      <td>...</td>\n",
              "      <td>63</td>\n",
              "      <td>42</td>\n",
              "      <td>43</td>\n",
              "      <td>76</td>\n",
              "      <td>80</td>\n",
              "      <td>76</td>\n",
              "      <td>73</td>\n",
              "      <td>69</td>\n",
              "      <td>64</td>\n",
              "      <td>59</td>\n",
              "      <td>53</td>\n",
              "      <td>54</td>\n",
              "      <td>52</td>\n",
              "      <td>54</td>\n",
              "      <td>57</td>\n",
              "      <td>60</td>\n",
              "      <td>66</td>\n",
              "      <td>71</td>\n",
              "      <td>77</td>\n",
              "      <td>85</td>\n",
              "      <td>89</td>\n",
              "      <td>93</td>\n",
              "      <td>102</td>\n",
              "      <td>106</td>\n",
              "      <td>105</td>\n",
              "      <td>99</td>\n",
              "      <td>85</td>\n",
              "      <td>62</td>\n",
              "      <td>34</td>\n",
              "      <td>22</td>\n",
              "      <td>29</td>\n",
              "      <td>36</td>\n",
              "      <td>68</td>\n",
              "      <td>91</td>\n",
              "      <td>85</td>\n",
              "      <td>93</td>\n",
              "      <td>97</td>\n",
              "      <td>99</td>\n",
              "      <td>107</td>\n",
              "      <td>118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Happy</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>13</td>\n",
              "      <td>41</td>\n",
              "      <td>56</td>\n",
              "      <td>62</td>\n",
              "      <td>67</td>\n",
              "      <td>87</td>\n",
              "      <td>95</td>\n",
              "      <td>62</td>\n",
              "      <td>65</td>\n",
              "      <td>70</td>\n",
              "      <td>80</td>\n",
              "      <td>107</td>\n",
              "      <td>127</td>\n",
              "      <td>149</td>\n",
              "      <td>153</td>\n",
              "      <td>150</td>\n",
              "      <td>165</td>\n",
              "      <td>168</td>\n",
              "      <td>177</td>\n",
              "      <td>187</td>\n",
              "      <td>176</td>\n",
              "      <td>167</td>\n",
              "      <td>152</td>\n",
              "      <td>128</td>\n",
              "      <td>130</td>\n",
              "      <td>149</td>\n",
              "      <td>149</td>\n",
              "      <td>146</td>\n",
              "      <td>130</td>\n",
              "      <td>139</td>\n",
              "      <td>139</td>\n",
              "      <td>143</td>\n",
              "      <td>134</td>\n",
              "      <td>105</td>\n",
              "      <td>78</td>\n",
              "      <td>56</td>\n",
              "      <td>36</td>\n",
              "      <td>...</td>\n",
              "      <td>19</td>\n",
              "      <td>16</td>\n",
              "      <td>11</td>\n",
              "      <td>8</td>\n",
              "      <td>46</td>\n",
              "      <td>47</td>\n",
              "      <td>46</td>\n",
              "      <td>47</td>\n",
              "      <td>48</td>\n",
              "      <td>51</td>\n",
              "      <td>55</td>\n",
              "      <td>55</td>\n",
              "      <td>56</td>\n",
              "      <td>76</td>\n",
              "      <td>85</td>\n",
              "      <td>84</td>\n",
              "      <td>84</td>\n",
              "      <td>77</td>\n",
              "      <td>86</td>\n",
              "      <td>86</td>\n",
              "      <td>80</td>\n",
              "      <td>79</td>\n",
              "      <td>82</td>\n",
              "      <td>84</td>\n",
              "      <td>76</td>\n",
              "      <td>62</td>\n",
              "      <td>57</td>\n",
              "      <td>62</td>\n",
              "      <td>61</td>\n",
              "      <td>62</td>\n",
              "      <td>59</td>\n",
              "      <td>66</td>\n",
              "      <td>80</td>\n",
              "      <td>74</td>\n",
              "      <td>69</td>\n",
              "      <td>99</td>\n",
              "      <td>28</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Fear</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>...</td>\n",
              "      <td>43</td>\n",
              "      <td>57</td>\n",
              "      <td>66</td>\n",
              "      <td>74</td>\n",
              "      <td>76</td>\n",
              "      <td>76</td>\n",
              "      <td>82</td>\n",
              "      <td>87</td>\n",
              "      <td>97</td>\n",
              "      <td>72</td>\n",
              "      <td>30</td>\n",
              "      <td>60</td>\n",
              "      <td>68</td>\n",
              "      <td>78</td>\n",
              "      <td>72</td>\n",
              "      <td>63</td>\n",
              "      <td>46</td>\n",
              "      <td>80</td>\n",
              "      <td>80</td>\n",
              "      <td>35</td>\n",
              "      <td>39</td>\n",
              "      <td>48</td>\n",
              "      <td>65</td>\n",
              "      <td>90</td>\n",
              "      <td>89</td>\n",
              "      <td>53</td>\n",
              "      <td>17</td>\n",
              "      <td>37</td>\n",
              "      <td>44</td>\n",
              "      <td>82</td>\n",
              "      <td>94</td>\n",
              "      <td>21</td>\n",
              "      <td>22</td>\n",
              "      <td>51</td>\n",
              "      <td>71</td>\n",
              "      <td>80</td>\n",
              "      <td>82</td>\n",
              "      <td>79</td>\n",
              "      <td>79</td>\n",
              "      <td>83</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 2305 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "  emotion  pixel_0  pixel_1  ...  pixel_2301  pixel_2302  pixel_2303\n",
              "0    Fear      231      212  ...          88         110         152\n",
              "1    Fear       55       55  ...          34          30          57\n",
              "2     Sad       20       17  ...          99         107         118\n",
              "3   Happy        4        2  ...           3           7          12\n",
              "4    Fear      255      255  ...          79          79          83\n",
              "\n",
              "[5 rows x 2305 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2OD2ROeB-EE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "seed_value =9\n",
        "tf.random.set_seed(seed_value)\n",
        "random.seed(seed_value)\n",
        "np.random.seed(seed_value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QVZ6qY4pRnK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "label=OneHotEncoder()\n",
        "dataX=df.drop(['emotion'],axis=1)\n",
        "dataY=df['emotion'].values\n",
        "dataY=dataY.reshape(-1,1)\n",
        "dataY=label.fit_transform(dataY)\n",
        "dataY=dataY.toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iEDEhitpmvs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(dataX, dataY, test_size=0.15,random_state=9,stratify =dataY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tkjz9XhndSwH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train -= np.mean(X_train, axis=0)\n",
        "X_train /= np.std(X_train, axis=0)\n",
        "\n",
        "X_test -= np.mean(X_test, axis=0)\n",
        "X_test /= np.std(X_test, axis=0)\n",
        "\n",
        "X_train = X_train.values\n",
        "X_test = X_test.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKquPtcKprxV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "b1b727cd-5f99-4368-e4d7-676f9780e1ce"
      },
      "source": [
        "width, height = 48, 48\n",
        "X_train = X_train.reshape(len(X_train),height,width)\n",
        "X_test = X_test.reshape(len(X_test),height,width)\n",
        "X_train = np.expand_dims(X_train,3)\n",
        "X_test = np.expand_dims(X_test,3)\n",
        "print(X_train.shape)\n",
        "y_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(9194, 48, 48, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9194, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocYxmujYpt3N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Conv2D,MaxPooling2D,BatchNormalization,Dropout,Flatten,Dense"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aIVyWNaeICE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(X_train.shape[1:])))\n",
        "model.add(Conv2D(64,kernel_size= (3, 3), activation='relu'))\n",
        "# model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "#2nd convolution layer\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "# model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "#3rd convolution layer\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "# model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "#fully connected neural networks\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(3, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlehFhfrqcvP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsdAqxgIpyb3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filepath=\"drive/My Drive/best_cnn_model.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, verbose=1, monitor='val_loss',save_best_only=True, mode='min')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkyCjfWFq7W-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#opt = keras.optimizers.Adam(learning_rate=0.05)\n",
        "model.compile(loss='categorical_crossentropy',optimizer='Adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7peZUr3KqMDX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "55b0c6c9-a705-4edf-b4d0-2f6498401e84"
      },
      "source": [
        "batch_size=128\n",
        "epochs=25\n",
        "cnn_history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, callbacks=[checkpoint],\n",
        "                            validation_data=(X_test, y_test),shuffle=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "72/72 [==============================] - ETA: 0s - loss: 1.0649 - accuracy: 0.4425\n",
            "Epoch 00001: val_loss improved from inf to 1.05024, saving model to drive/My Drive/best_cnn_model.hdf5\n",
            "72/72 [==============================] - 168s 2s/step - loss: 1.0649 - accuracy: 0.4425 - val_loss: 1.0502 - val_accuracy: 0.4553\n",
            "Epoch 2/25\n",
            "72/72 [==============================] - ETA: 0s - loss: 1.0449 - accuracy: 0.4649\n",
            "Epoch 00002: val_loss improved from 1.05024 to 1.03862, saving model to drive/My Drive/best_cnn_model.hdf5\n",
            "72/72 [==============================] - 172s 2s/step - loss: 1.0449 - accuracy: 0.4649 - val_loss: 1.0386 - val_accuracy: 0.4627\n",
            "Epoch 3/25\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.9919 - accuracy: 0.5059\n",
            "Epoch 00003: val_loss improved from 1.03862 to 0.95810, saving model to drive/My Drive/best_cnn_model.hdf5\n",
            "72/72 [==============================] - 171s 2s/step - loss: 0.9919 - accuracy: 0.5059 - val_loss: 0.9581 - val_accuracy: 0.5397\n",
            "Epoch 4/25\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.9140 - accuracy: 0.5583\n",
            "Epoch 00004: val_loss improved from 0.95810 to 0.85801, saving model to drive/My Drive/best_cnn_model.hdf5\n",
            "72/72 [==============================] - 170s 2s/step - loss: 0.9140 - accuracy: 0.5583 - val_loss: 0.8580 - val_accuracy: 0.6032\n",
            "Epoch 5/25\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.8392 - accuracy: 0.6059\n",
            "Epoch 00005: val_loss improved from 0.85801 to 0.82349, saving model to drive/My Drive/best_cnn_model.hdf5\n",
            "72/72 [==============================] - 173s 2s/step - loss: 0.8392 - accuracy: 0.6059 - val_loss: 0.8235 - val_accuracy: 0.6118\n",
            "Epoch 6/25\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.7968 - accuracy: 0.6287\n",
            "Epoch 00006: val_loss improved from 0.82349 to 0.80519, saving model to drive/My Drive/best_cnn_model.hdf5\n",
            "72/72 [==============================] - 169s 2s/step - loss: 0.7968 - accuracy: 0.6287 - val_loss: 0.8052 - val_accuracy: 0.6198\n",
            "Epoch 7/25\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.7641 - accuracy: 0.6439\n",
            "Epoch 00007: val_loss improved from 0.80519 to 0.74479, saving model to drive/My Drive/best_cnn_model.hdf5\n",
            "72/72 [==============================] - 172s 2s/step - loss: 0.7641 - accuracy: 0.6439 - val_loss: 0.7448 - val_accuracy: 0.6562\n",
            "Epoch 8/25\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.7332 - accuracy: 0.6571\n",
            "Epoch 00008: val_loss improved from 0.74479 to 0.73124, saving model to drive/My Drive/best_cnn_model.hdf5\n",
            "72/72 [==============================] - 168s 2s/step - loss: 0.7332 - accuracy: 0.6571 - val_loss: 0.7312 - val_accuracy: 0.6611\n",
            "Epoch 9/25\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.7154 - accuracy: 0.6679\n",
            "Epoch 00009: val_loss improved from 0.73124 to 0.72789, saving model to drive/My Drive/best_cnn_model.hdf5\n",
            "72/72 [==============================] - 173s 2s/step - loss: 0.7154 - accuracy: 0.6679 - val_loss: 0.7279 - val_accuracy: 0.6661\n",
            "Epoch 10/25\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.6955 - accuracy: 0.6760\n",
            "Epoch 00010: val_loss improved from 0.72789 to 0.69612, saving model to drive/My Drive/best_cnn_model.hdf5\n",
            "72/72 [==============================] - 168s 2s/step - loss: 0.6955 - accuracy: 0.6760 - val_loss: 0.6961 - val_accuracy: 0.6981\n",
            "Epoch 11/25\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.6744 - accuracy: 0.6875\n",
            "Epoch 00011: val_loss did not improve from 0.69612\n",
            "72/72 [==============================] - 169s 2s/step - loss: 0.6744 - accuracy: 0.6875 - val_loss: 0.7018 - val_accuracy: 0.6784\n",
            "Epoch 12/25\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.6541 - accuracy: 0.6986\n",
            "Epoch 00012: val_loss improved from 0.69612 to 0.68183, saving model to drive/My Drive/best_cnn_model.hdf5\n",
            "72/72 [==============================] - 168s 2s/step - loss: 0.6541 - accuracy: 0.6986 - val_loss: 0.6818 - val_accuracy: 0.6919\n",
            "Epoch 13/25\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.6449 - accuracy: 0.7058\n",
            "Epoch 00013: val_loss did not improve from 0.68183\n",
            "72/72 [==============================] - 170s 2s/step - loss: 0.6449 - accuracy: 0.7058 - val_loss: 0.7007 - val_accuracy: 0.6753\n",
            "Epoch 14/25\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.6263 - accuracy: 0.7142\n",
            "Epoch 00014: val_loss did not improve from 0.68183\n",
            "72/72 [==============================] - 167s 2s/step - loss: 0.6263 - accuracy: 0.7142 - val_loss: 0.6926 - val_accuracy: 0.6993\n",
            "Epoch 15/25\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.6057 - accuracy: 0.7237\n",
            "Epoch 00015: val_loss improved from 0.68183 to 0.66931, saving model to drive/My Drive/best_cnn_model.hdf5\n",
            "72/72 [==============================] - 172s 2s/step - loss: 0.6057 - accuracy: 0.7237 - val_loss: 0.6693 - val_accuracy: 0.6999\n",
            "Epoch 16/25\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.6008 - accuracy: 0.7278\n",
            "Epoch 00016: val_loss did not improve from 0.66931\n",
            "72/72 [==============================] - 168s 2s/step - loss: 0.6008 - accuracy: 0.7278 - val_loss: 0.6897 - val_accuracy: 0.6858\n",
            "Epoch 17/25\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.5798 - accuracy: 0.7381\n",
            "Epoch 00017: val_loss improved from 0.66931 to 0.65920, saving model to drive/My Drive/best_cnn_model.hdf5\n",
            "72/72 [==============================] - 172s 2s/step - loss: 0.5798 - accuracy: 0.7381 - val_loss: 0.6592 - val_accuracy: 0.7043\n",
            "Epoch 18/25\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.5726 - accuracy: 0.7427\n",
            "Epoch 00018: val_loss did not improve from 0.65920\n",
            "72/72 [==============================] - 168s 2s/step - loss: 0.5726 - accuracy: 0.7427 - val_loss: 0.6727 - val_accuracy: 0.7135\n",
            "Epoch 19/25\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.5544 - accuracy: 0.7461\n",
            "Epoch 00019: val_loss did not improve from 0.65920\n",
            "72/72 [==============================] - 169s 2s/step - loss: 0.5544 - accuracy: 0.7461 - val_loss: 0.6827 - val_accuracy: 0.6981\n",
            "Epoch 20/25\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.5448 - accuracy: 0.7538\n",
            "Epoch 00020: val_loss did not improve from 0.65920\n",
            "72/72 [==============================] - 169s 2s/step - loss: 0.5448 - accuracy: 0.7538 - val_loss: 0.6706 - val_accuracy: 0.7092\n",
            "Epoch 21/25\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.5225 - accuracy: 0.7637\n",
            "Epoch 00021: val_loss did not improve from 0.65920\n",
            "72/72 [==============================] - 169s 2s/step - loss: 0.5225 - accuracy: 0.7637 - val_loss: 0.7014 - val_accuracy: 0.6993\n",
            "Epoch 22/25\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.5130 - accuracy: 0.7716\n",
            "Epoch 00022: val_loss did not improve from 0.65920\n",
            "72/72 [==============================] - 168s 2s/step - loss: 0.5130 - accuracy: 0.7716 - val_loss: 0.6714 - val_accuracy: 0.7092\n",
            "Epoch 23/25\n",
            "49/72 [===================>..........] - ETA: 51s - loss: 0.5075 - accuracy: 0.7805"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQNC14Yzv5jH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
